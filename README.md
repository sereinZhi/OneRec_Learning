# OneRec_Learning
Learning OneRec

首先了解数据集：
📊 RecIF-Bench
我们提出 RecIF-Bench，旨在严格评估指令遵循和领域特定推荐之间的协同作用。它将 8 项不同的任务组织成一个四层能力层级结构：
第 0 层：语义对齐（项目理解）
第 1 层：基础预测（短视频推荐、广告推荐、产品推荐、标签预测）
第 2 层：指令遵循（交互式推荐、标签条件推荐）
第 3 层：推理（推荐解释）
该基准测试聚合了来自三个领域的数据：短视频（内容）、广告（商业）和产品（电子商务）。

看的不是很明白，先看看有哪些数据：
<img width="2376" height="503" alt="image" src="https://github.com/user-attachments/assets/2740a351-cb8c-44b9-bd36-a26db6fce807" />
怎么理解？通用预训练、通用指令微调以及推荐领域指令对齐。
进一步理解：
1. OpenOneRec-General-Pretrain (通用预训练)这是模型的“知识底座”。在训练推荐基础模型时，如果只投喂推荐数据，模型会丧失逻辑推理能力。
用途：在预训练的第二阶段加入，包含大量通用领域的文本数据。
目标：维持模型的通用推理能力（General Reasoning），使其在处理推荐逻辑时依然具备常识。
 2. OpenOneRec-General-SFT (通用指令微调)在模型学会了知识后，需要教它“怎么听话”。
用途：从通用的 SFT 数据集中采样（如技术报告中提到的 200K 条目），涵盖问答、对话等任务。
目标：恢复和增强模型的指令遵循（Instruction-following）能力，确保它能理解“请帮我推荐...”这类句式的结构。
 3. OpenOneRec-RecIF (推荐指令对齐)这是最关键的“专业课”数据集，RecIF 代表 Recommendation Instruction Following。
用途：包含海量的用户行为序列与自然语言指令的配对数据。它将推荐系统的 Item ID 映射到文本空间。
内容：涵盖了 8 类推荐任务，如：
基础推荐：根据历史看什么，推荐下一个。
约束推荐：推荐特定类别（如“只要喜剧片”）的内容。
解释性推荐：告诉用户为什么要推荐这个。
实践建议
模型开发：如果你在复现 OneRec 系列模型，应按照 Pretrain -> SFT -> RecIF 的顺序使用这些数据。
数据采样：General-Pretrain 很大，如果算力有限，可以参考技术报告中的比例进行下采样。
评估基准：训练完成后，建议使用配套的 RecIF-Bench 评估模型在推荐领域的实际表现。
